{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "needed-costume",
   "metadata": {},
   "source": [
    "![CC](https://i.creativecommons.org/l/by/4.0/88x31.png)\n",
    "\n",
    "This work is licensed under a [Creative Commons Attribution 4.0 International License](http://creativecommons.org/licenses/by/4.0/).\n",
    "\n",
    "# End-to-end ML project with OpenFOAM and PyTorch\n",
    "\n",
    "## Part I: creating a parameter study\n",
    "\n",
    "### Inspecting and running the base simulation\n",
    "\n",
    "The base simulation for data generation is a 1D channel flow. The simulation folder is located at *test_cases/boundary_layer_1D*. Create a copy of the test case in the exercise folder and run the simulation:\n",
    "```\n",
    "# starting from the repository's top level\n",
    "source setup-env\n",
    "cp -r test_cases/boundary_layer_1D/ exercises/\n",
    "cd exercises/boundary_layer_1D\n",
    "./Allrun\n",
    "```\n",
    "The simulation completes within a few minutes. In the meantime, try answering the following questions about the setup:\n",
    "\n",
    "- How many cells does the mesh consist of? Tip: use `source $ML_CFD_BASE/RunFunctions` and `runApplication checkMesh`.\n",
    "- What are the boundary conditions for $U$?\n",
    "- What is the driving force for this flow? Tip: check the dictionary *system/fvOptions* and consult the [documentation](https://www.openfoam.com/documentation/guides/v2112/doc/index.html).\n",
    "\n",
    "Once the simulation is finished, open the case in ParaView and complete the following tasks:\n",
    "\n",
    "- load the final flow state and visualize the velocity profile using the *Glyph* filter\n",
    "- visualize individual patches by unselecting the *internalMesh* and selecting only individual patches under *Mesh Regions* in the left properties panel; check again the boundary condition defined for each patch in *0/U*\n",
    "\n",
    "Close ParaView and reset the simulation by running `./Allclean`. The next goal is to perform the same simulation with increased Reynolds number. The following steps guide you to the modified setup:\n",
    "\n",
    "- double the Reynolds number by doubling the mean velocity along the channel\n",
    "- in *system/controlDict*, adjust the time step such that the Courant number remains roughly constant\n",
    "- considering a dimensionless time of $\\tilde{t} = t\\bar{U}/(2\\delta)$, where $\\bar{U}$ is the mean velocity along the channel and $2\\delta$ is the channel height, modify the end time in *system/controlDict* such that the same amount of dimensionless time units as before is simulated\n",
    "- re-run the simulation and inspect the results in ParaView\n",
    "\n",
    "## Performing the parameter variation\n",
    "\n",
    "The script *parameter_variation_1d.py* in *test_cases* automates the manual modifications of the simulation setup conducted in the previous step. To perform the parameter variation, make a copy of the script in the exercise folder:\n",
    "```\n",
    "# assuming you are at the repository's top level\n",
    "source setup-env\n",
    "cp test_cases/parameter_variation_1d.py exercises/\n",
    "```\n",
    "Now open the script and inspect the implemented functions. Try answering the following questions:\n",
    "\n",
    "- How many simulations are performed in total?\n",
    "- How many simulations are performed at the same time?\n",
    "- Which parameter(s) in which file(s) of the base setup is/are modified?\n",
    "- Where are the modified simulations stored?\n",
    "\n",
    "Your workstation or laptop might be equipped with fewer compute cores than the script assumes. Running multiple simulations at the same time on shared resources slows down the computations unnecessarily. To determine the number of CPU cores available on your machine, run the command `lscpu` and search for the line *Core(s) per socket ...* in the output. You should not run more simulations simultaneously than cores are available (each simulation runs only on a single core). Modify the script accordingly, and divide the parameter space into 10 to 30 sections (this number determines how many simulations are performed). To start the parameter study, start the Python environment, make sure the script is executable, and run the script:\n",
    "```\n",
    "# assuming you are at the repository's top level\n",
    "source ml-cfd/bin/activate\n",
    "cd exercises\n",
    "chmod +x parameter_variation_1d.py\n",
    "python parameter_variation_1d.py\n",
    "```\n",
    "Depending on the available resources and the overall number of simulations, this computation should take about 10-30min. Once all simulations are complete, open a Jupyter notebook and use the following code snippet to load and visualize the velocity profiles:\n",
    "```\n",
    "from glob import glob\n",
    "from os.path import join\n",
    "import torch as pt\n",
    "import matplotlib.pyplot as plt\n",
    "from flowtorch.data import FOAMDataloader\n",
    "\n",
    "#\n",
    "# adjust the path if necessary\n",
    "#\n",
    "cases = glob(\"./boundary_layer_1D_variation/Ub_*\")\n",
    "\n",
    "cases = sorted(cases, key=lambda case: float(case.split(\"_\")[-1]))\n",
    "loader = FOAMDataloader(cases[0])\n",
    "y = loader.vertices[:, 1]\n",
    "u_x = pt.zeros((y.shape[0], len(cases)))\n",
    "for i, case in enumerate(cases):\n",
    "    loader = FOAMDataloader(case)\n",
    "    u_x[:, i] = loader.load_snapshot(\"U\", loader.write_times[-1])[:, 0]\n",
    "\n",
    "Ubar = pt.tensor([float(case.split(\"_\")[-1]) for case in cases])\n",
    "print(\"Shape of data matrix: \", u_x.shape)\n",
    "\n",
    "# creating a plot\n",
    "delta, nu = 0.5, 1.0e-5\n",
    "Re = pt.tensor([Ub.item()*2*delta/nu for Ub in Ubar])\n",
    "for i, Ub in enumerate(Ubar):\n",
    "    plt.plot(u_x[:, i], y, label=r\"$Re={:1.0f}$\".format(round(Re[i].item(), 0)))\n",
    "plt.xlabel(r\"$u_x$\")\n",
    "plt.ylabel(r\"$y$\")\n",
    "plt.xlim(0.0, 1.1)\n",
    "plt.ylim(-0.01, 0.5)\n",
    "plt.legend(loc=\"upper center\", ncol=4, bbox_to_anchor=[0.5, 1.3])\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0712de16",
   "metadata": {},
   "source": [
    "## Part II: creating a model for the streamwise velocity\n",
    "\n",
    "## Direct learning approach\n",
    "\n",
    "Following the lecture notebook:\n",
    "\n",
    "- compare the velocity profiles against Spalding's function\n",
    "- split, reshape, and normalize the data\n",
    "- train a baseline model and visualize the $L_2$ norm computed on training, validation and testing data; tip: start with a rather simple model and training routine for the baseline model and only add more complex techniques or architectures once the simple workflow is established\n",
    "- plot the predictions against the original data\n",
    "\n",
    "## Hyperparameter tuning\n",
    "\n",
    "In the next step, we try to tune the ML model. Vary the following hyperparameters and try to minimize the prediction error:\n",
    "\n",
    "- number of neurons per layer\n",
    "- number of hidden layers\n",
    "\n",
    "Take the best model you found, compare the prediction against the original data.\n",
    "\n",
    "## Leveraging Spalding's function\n",
    "\n",
    "The good agreement of our data with Spalding's function might have triggered already the idea that we should be able to use this relation to simplify the modeling. The following steps guide you through the approach:\n",
    "\n",
    "- for each Reynolds number, extract the friction velocity $u_\\tau$ and plot $\\tilde{u}_\\tau = u_\\tau/\\bar{U}$ against $Re$; compare the simulation results against the empirical formula $\\tilde{u}_\\tau = \\frac{0.169}{Re^{0.115}}$\n",
    "- transform the original data using the $\\tilde{u}_\\tau$ formula above as follows:  \n",
    "  - transform $u_x$ to $u^+$\n",
    "  - transform the distance $y$ to $\\tilde{y} = \\mathrm{log}(y^+)$  \n",
    "  - select profiles for training, validation, and testing\n",
    "  - reshape and normalize the data  \n",
    "\n",
    "Since the new model has one feature less, the modified reshape function should look as follows:\n",
    "\n",
    "```\n",
    "def reshape_data(u_plus, y_plus):\n",
    "    data = pt.zeros((u_plus.shape[0]*u_plus.shape[1], 2))\n",
    "    for i in range(u_plus.shape[1]):\n",
    "        start, end = i*u_plus.shape[0], (i+1)*u_plus.shape[0]\n",
    "        data[start:end, 0] = u_plus[:, i]\n",
    "        data[start:end, 1] = y_plus[:, i]\n",
    "    return data\n",
    "```\n",
    "\n",
    "PyTorch models expect the input to have at least two dimensions. Therefore, both the feature and label tensors should have the shape $N_s\\times 1$ rather than $N_s$ ($N_s$ is the number of samples). This additional reshaping is easily done with `Tensor.unsqueeze(-1)`:\n",
    "```\n",
    "train_dataset = pt.utils.data.TensorDataset(\n",
    "    feature_scaler.scale(train_tensor[:, 1]).unsqueeze(-1), label_scaler.scale(train_tensor[:, 0]).unsqueeze(-1)\n",
    ")\n",
    "```\n",
    "\n",
    "Now we are ready to train and evaluate the new model:\n",
    "- create and train a model $u^+ = f_{\\mathbf{\\theta}}(\\tilde{y})$\n",
    "- make predictions for all Reynolds numbers:  \n",
    "  - compute $u_\\tau$ based on $Re$ and $\\bar{U}$\n",
    "  - compute $\\tilde{y}$ and scale\n",
    "  - make a prediction for the scaled $u^+$, re-scale, and multiply by $u_\\tau$\n",
    "- compare the predictions against the true velocity profiles\n",
    "\n",
    "The additional transformations and scaling increase the inference complexity. To avoid prediction errors resulting from missing scaling or normalization, it is possible to hide these steps in a top-level model. The following exercises will demonstrate how such composite models are created.\n",
    "\n",
    "**Congratulations! This completes the fourth and fifth exercise sessions.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe063260",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('ml-cfd': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "cbb112329e189d437c2dd20cb32069784312f3fb0cc6892ecadf8f4f78f994e7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
