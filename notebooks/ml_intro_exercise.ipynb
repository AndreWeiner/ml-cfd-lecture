{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "needed-costume",
   "metadata": {},
   "source": [
    "![CC](https://i.creativecommons.org/l/by/4.0/88x31.png)\n",
    "\n",
    "This work is licensed under a [Creative Commons Attribution 4.0 International License](http://creativecommons.org/licenses/by/4.0/).\n",
    "\n",
    "# End-to-end ML project with OpenFOAM and PyTorch\n",
    "\n",
    "**Note: this exercise still needs to be updated for the winter term 2022/2023.**\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this exercise, we generate and work with the same data as in lecture 3. Many of the tasks can be completed by copying and modifying code snippets from the lecture notebook. Beyond the steps covered in the lecture, we will simplify the ML problem by leveraging a coordinate transformation coming from turbulence modeling.\n",
    "\n",
    "## Generating the data\n",
    "\n",
    "To generate the data, copy the *parameter_variation_1d.py* into the exercise folder:\n",
    "```\n",
    "# assuming you are at the top-level of the lecture repository\n",
    "cp test_cases/parameter_variation_1d.py exercises/\n",
    "```\n",
    "Now open the script and inspect the implemented functions. Try answering the following questions:\n",
    "\n",
    "1. Where is the OpenFOAM base simulation located?\n",
    "2. How many simulations are performed in total?\n",
    "3. How many simulations are performed at the same time?\n",
    "4. Which parameter(s) in which file(s) of the base setup is/are modified?\n",
    "\n",
    "Your workstation or laptop might be equipped with fewer compute cores than the script assumes. Running multiple simulations at the same time on shared resources slows down the computations unnecessarily. To determine the number of CPU cores available on your machine, run the command `lscpu` and search for the line *Core(s) per socket ...* in the output. You should not run more simulations simultaneously than cores are available (each simulation runs only on a single core). Modify the script accordingly, and divide the parameter space into 10 to 30 sections (this number determines how many simulations are performed). To start the parameter study, make sure the script is executable and run the script:\n",
    "```\n",
    "cd exercises\n",
    "chmod +x parameter_variation_1d.py\n",
    "./parameter_variation_1d.py\n",
    "```\n",
    "Depending on the available resources and the overall number of simulations, this computation should take about 10-30min.\n",
    "\n",
    "## Direct learning approach\n",
    "\n",
    "Following the lecture notebook:\n",
    "\n",
    "- load and visualize the data\n",
    "- compare the velocity profiles against Spalding's function\n",
    "- reshape, split, and normalize the data\n",
    "- train a baseline model and evaluate the $L_2$ and $L_\\infty$ norms on the test data\n",
    "- compare the predictions of the best model against the original data\n",
    "- visualize the prediction errors by an additional method of your choice\n",
    "\n",
    "## Hyperparameter tuning\n",
    "\n",
    "In the next step, we try to tune the ML model. Vary the following hyperparameters and try to minimize the prediction error:\n",
    "\n",
    "- number of training epochs\n",
    "- learning rate\n",
    "- number of neurons per layer\n",
    "- number of hidden layers\n",
    "- activation function\n",
    "- repeated training runs\n",
    "\n",
    "Take the best model you found, compare the prediction against the original data, and visualize the prediction error.\n",
    "\n",
    "## Leveraging Spalding's function\n",
    "\n",
    "The good agreement of our data with Spalding's function might have triggered already the idea that we should somehow be able to use this relation. It might not be exactly clear yet how leverage this knowledge from turbulence modeling, but the following steps will guide you there:\n",
    "\n",
    "- for each Reynolds number, extract the friction velocity $u_\\tau$ and plot $u_\\tau$ against $Re$\n",
    "- create a model of your choice for the relation $u_\\tau = f(Re)$; if you think that a simpler model than a neural network will do the task, use a simpler model\n",
    "- transform the original data using the $u_\\tau$ model as follows:  \n",
    "  - transform $U_x$ to $u^+$\n",
    "  - transform the distance $y$ to $\\tilde{y} = \\mathrm{log}(y^+)$  \n",
    "  - normalize the new data  \n",
    "- create a new model for the relation $u^+ = f(\\tilde{y}, Re$\n",
    "- make a prediction $\\hat{u}^+$ based on the test data\n",
    "- transform $\\hat{u}^+$ into $\\hat{U}_x$ using the $u_\\tau$ model\n",
    "- compare the model performance to the best model obtained with the direct training approach\n",
    "\n",
    "In one of the next lectures, you will learn how to hide the composition of multiple models in a single top-level model.\n",
    "\n",
    "**Congratulations! This completes the third exercise session.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe063260",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
