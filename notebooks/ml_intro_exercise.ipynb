{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "needed-costume",
   "metadata": {},
   "source": [
    "![CC](https://i.creativecommons.org/l/by/4.0/88x31.png)\n",
    "\n",
    "This work is licensed under a [Creative Commons Attribution 4.0 International License](http://creativecommons.org/licenses/by/4.0/).\n",
    "\n",
    "# End-to-end ML project with OpenFOAM and PyTorch\n",
    "\n",
    "## Part I: creating a parameter study\n",
    "\n",
    "### Inspecting and running the base simulation\n",
    "\n",
    "The base simulation for data generation is a 1D channel flow. The simulation folder is located at *test_cases/boundary_layer_1D*. Create a copy of the test case in the exercise folder and run the simulation:\n",
    "```\n",
    "# starting from the repository's top level\n",
    "source setup-env\n",
    "cp -r test_cases/boundary_layer_1D/ exercises/\n",
    "cd exercises/boundary_layer_1D\n",
    "./Allrun\n",
    "```\n",
    "The simulation completes within a few minutes. In the meantime, try answering the following questions about the setup:\n",
    "\n",
    "- How many cells does the mesh consist of? Tip: use `source $ML_CFD_BASE/RunFunctions` and `runApplication checkMesh`.\n",
    "- What are the boundary conditions for $U$?\n",
    "- What is the driving force for this flow? Tip: check the dictionary *system/fvOptions* and consult the [documentation](https://www.openfoam.com/documentation/guides/v2112/doc/index.html).\n",
    "\n",
    "Once the simulation is finished, open the case in ParaView and complete the following tasks:\n",
    "\n",
    "- load the final flow state and visualize the velocity profile using the *Glyph* filter\n",
    "- visualize individual patches by unselecting the *internalMesh* and selecting only individual patches under *Mesh Regions* in the left properties panel; check again the boundary condition defined for each patch in *0/U*\n",
    "\n",
    "Close ParaView and reset the simulation by running `./Allclean`. The next goal is to perform the same simulation with increased Reynolds number. The following steps guide you to the modified setup:\n",
    "\n",
    "- double the Reynolds number by doubling the mean velocity along the channel\n",
    "- in *system/controlDict*, adjust the time step such that the Courant number remains roughly constant\n",
    "- considering a dimensionless time of $\\tilde{t} = t\\bar{U}/(2\\delta)$, where $\\bar{U}$ is the mean velocity along the channel and $2\\delta$ is the channel height, modify the end time in *system/controlDict* such that the same amount of dimensionless time units as before is simulated\n",
    "- re-run the simulation and inspect the results in ParaView\n",
    "\n",
    "## Performing the parameter variation\n",
    "\n",
    "The script *parameter_variation_1d.py* in *test_cases* automates the manual modifications of the simulation setup conducted in the previous step. To perform the parameter variation, make a copy of the script in the exercise folder:\n",
    "```\n",
    "# assuming you are at the repository's top level\n",
    "source setup-env\n",
    "cp test_cases/parameter_variation_1d.py exercises/\n",
    "```\n",
    "Now open the script and inspect the implemented functions. Try answering the following questions:\n",
    "\n",
    "- How many simulations are performed in total?\n",
    "- How many simulations are performed at the same time?\n",
    "- Which parameter(s) in which file(s) of the base setup is/are modified?\n",
    "- Where are the modified simulations stored?\n",
    "\n",
    "Your workstation or laptop might be equipped with fewer compute cores than the script assumes. Running multiple simulations at the same time on shared resources slows down the computations unnecessarily. To determine the number of CPU cores available on your machine, run the command `lscpu` and search for the line *Core(s) per socket ...* in the output. You should not run more simulations simultaneously than cores are available (each simulation runs only on a single core). Modify the script accordingly, and divide the parameter space into 10 to 30 sections (this number determines how many simulations are performed). To start the parameter study, start the Python environment, make sure the script is executable, and run the script:\n",
    "```\n",
    "# assuming you are at the repository's top level\n",
    "source ml-cfd/bin/activate\n",
    "cd exercises\n",
    "chmod +x parameter_variation_1d.py\n",
    "python parameter_variation_1d.py\n",
    "```\n",
    "Depending on the available resources and the overall number of simulations, this computation should take about 10-30min. Once all simulations are complete, open a Jupyter notebook and use the following code snippet to load and visualize the velocity profiles:\n",
    "```\n",
    "from glob import glob\n",
    "from os.path import join\n",
    "import torch as pt\n",
    "import matplotlib.pyplot as plt\n",
    "from flowtorch.data import FOAMDataloader\n",
    "\n",
    "#\n",
    "# adjust the path if necessary\n",
    "#\n",
    "cases = glob(\"./boundary_layer_1D_variation/Ub_*\")\n",
    "\n",
    "cases = sorted(cases, key=lambda case: float(case.split(\"_\")[-1]))\n",
    "loader = FOAMDataloader(cases[0])\n",
    "y = loader.vertices[:, 1]\n",
    "u_x = pt.zeros((y.shape[0], len(cases)))\n",
    "for i, case in enumerate(cases):\n",
    "    loader = FOAMDataloader(case)\n",
    "    u_x[:, i] = loader.load_snapshot(\"U\", loader.write_times[-1])[:, 0]\n",
    "\n",
    "Ubar = pt.tensor([float(case.split(\"_\")[-1]) for case in cases])\n",
    "print(\"Shape of data matrix: \", u_x.shape)\n",
    "\n",
    "# creating a plot\n",
    "delta, nu = 0.5, 1.0e-5\n",
    "Re = pt.tensor([Ub.item()*2*delta/nu for Ub in Ubar])\n",
    "for i, Ub in enumerate(Ubar):\n",
    "    plt.plot(u_x[:, i], y, label=r\"$Re={:1.0f}$\".format(round(Re[i].item(), 0)))\n",
    "plt.xlabel(r\"$u_x$\")\n",
    "plt.ylabel(r\"$y$\")\n",
    "plt.xlim(0.0, 1.1)\n",
    "plt.ylim(-0.01, 0.5)\n",
    "plt.legend(loc=\"upper center\", ncol=4, bbox_to_anchor=[0.5, 1.3])\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0712de16",
   "metadata": {},
   "source": [
    "## Part II: creating a model for the streamwise velocity\n",
    "\n",
    "**Note: this part of the exercise still needs to be updated for the winter term 2022/2023.**\n",
    "\n",
    "## Direct learning approach\n",
    "\n",
    "Following the lecture notebook:\n",
    "\n",
    "- load and visualize the data\n",
    "- compare the velocity profiles against Spalding's function\n",
    "- reshape, split, and normalize the data\n",
    "- train a baseline model and evaluate the $L_2$ and $L_\\infty$ norms on the test data\n",
    "- compare the predictions of the best model against the original data\n",
    "- visualize the prediction errors by an additional method of your choice\n",
    "\n",
    "## Hyperparameter tuning\n",
    "\n",
    "In the next step, we try to tune the ML model. Vary the following hyperparameters and try to minimize the prediction error:\n",
    "\n",
    "- number of training epochs\n",
    "- learning rate\n",
    "- number of neurons per layer\n",
    "- number of hidden layers\n",
    "- activation function\n",
    "- repeated training runs\n",
    "\n",
    "Take the best model you found, compare the prediction against the original data, and visualize the prediction error.\n",
    "\n",
    "## Leveraging Spalding's function\n",
    "\n",
    "The good agreement of our data with Spalding's function might have triggered already the idea that we should somehow be able to use this relation. It might not be exactly clear yet how leverage this knowledge from turbulence modeling, but the following steps will guide you there:\n",
    "\n",
    "- for each Reynolds number, extract the friction velocity $u_\\tau$ and plot $u_\\tau$ against $Re$\n",
    "- create a model of your choice for the relation $u_\\tau = f(Re)$; if you think that a simpler model than a neural network will do the task, use a simpler model\n",
    "- transform the original data using the $u_\\tau$ model as follows:  \n",
    "  - transform $U_x$ to $u^+$\n",
    "  - transform the distance $y$ to $\\tilde{y} = \\mathrm{log}(y^+)$  \n",
    "  - normalize the new data  \n",
    "- create a new model for the relation $u^+ = f(\\tilde{y}, Re$\n",
    "- make a prediction $\\hat{u}^+$ based on the test data\n",
    "- transform $\\hat{u}^+$ into $\\hat{U}_x$ using the $u_\\tau$ model\n",
    "- compare the model performance to the best model obtained with the direct training approach\n",
    "\n",
    "In one of the next lectures, you will learn how to hide the composition of multiple models in a single top-level model.\n",
    "\n",
    "**Congratulations! This completes the third and fourth exercise sessions.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe063260",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('ml-cfd': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "cbb112329e189d437c2dd20cb32069784312f3fb0cc6892ecadf8f4f78f994e7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
